# -*- coding: utf-8 -*-
"""Amazon_Product_Review_Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UW4x2XUcgdXv6Vw3giejZPXr_d-z_ZVo
"""

# Commented out IPython magic to ensure Python compatibility.
# Importing necessary paackages
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

import re
import nltk
from nltk.corpus import stopwords 
from nltk.stem.porter import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.stem.wordnet import WordNetLemmatizer
import string

from sklearn.model_selection import train_test_split,KFold
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.pipeline import Pipeline
import xgboost as xgb 
seed = 4353

#Text processing requisites
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

#Load train and test data for Product Reviews Analysis
train = pd.read_csv('train_data.csv')
test = pd.read_csv('test_data.csv')

print('Number of rows and columns in train data set',train.shape)

# Convert column names to uppercase
train.columns = train.columns.str.upper()

train.head()

print('Number of rows and columns in test data set',test.shape)
test.columns = test.columns.str.upper()
test.head()

# Check for any missing values in the training dataset
train.isnull().sum()

#Records corresponding to entries from training dataset in 'REVIEWS.TITLE' column with missing values
train[train['REVIEWS.TITLE'].isnull() == True]

# We shall not drop 'REVIEWS.TITLE' with missing data because we can still get useful information from 'REVIEWS.TEXT'
# Replace 'REVIEWS.TITLE' missing data with an empty string

train['REVIEWS.TITLE'].fillna(" " , inplace=True)

#Check if the sentiment outcomes are balanced / imbalanced
Sentiment_Counts = train['SENTIMENT'].value_counts()
Sentiment_Counts

"""The training dataset is imbalanced"""

#Visualization using Bar plot (matplotlib)
Sentiment_Counts.plot(kind= 'bar')

#Visualization using Bar plot (seaborn)
plt.figure(figsize=(14,5))
sns.countplot(train['SENTIMENT'],)
plt.xlabel('Sentiments')
plt.title('Sentiment distribution')
plt.show()

#Check for missing values in test data
test.isnull().sum()

# Replace 'REVIEWS.TITLE' missing data with an empty string
test['REVIEWS.TITLE'].fillna(" " , inplace=True)

#Creating Train and Test datasets with only the product reviews (Complete Review = Review Text + Review Title)

X_train = train['REVIEWS.TEXT'] + ' ' + train['REVIEWS.TITLE']
y_train = train['SENTIMENT']

X_test = test['REVIEWS.TEXT'] + ' ' + test['REVIEWS.TITLE']
y_test = test['SENTIMENT']

X_train[0]

X_train_df = pd.DataFrame(data= X_train)
X_train_df.columns = ['Review']
X_train_df.head()

"""# Text Preprocessing"""

string.punctuation # contains all the punctions which are to be removed from the reviews

def X_final(X_train_data, X_test_data):

  #Function for removing punctuations
  def remove_punctuations_from_string(X_data):
      string1 = X_data.lower() #changing to lower case
      translation_table = dict.fromkeys(map(ord, string.punctuation), ' ') #creating dictionary of punc & None
      string2 = string1.translate(translation_table) #translating 
      return string2

  X_train_data_clear_punct = []
  for i in range(0, len(X_train_data)):
    test_string = remove_punctuations_from_string(X_train_data[i])
    X_train_data_clear_punct.append(test_string)

  X_test_data_clear_punct = []
  for i in range(0, len(X_test_data)):
    test_string = remove_punctuations_from_string(X_test_data[i])
    X_test_data_clear_punct.append(test_string)


  #Function for removing stopwords
  def remove_stopwords_from_string(X_data):
      pattern = re.compile(r'\b(' + r'|'.join(stopwords.words('english')) + r')\b\s*') #compiling all stopwords.
      string2 = pattern.sub('', X_data) #replacing the occurrences of stopwords
      return string2

  X_train_data_clear_stopwords = []
  for i in range(0, len(X_train_data)):
    test_string = remove_stopwords_from_string(X_train_data_clear_punct[i])
    X_train_data_clear_stopwords.append(test_string)

  X_test_data_clear_stopwords = []
  for i in range(0, len(X_test_data)):
    test_string = remove_stopwords_from_string(X_test_data_clear_punct[i])
    X_test_data_clear_stopwords.append(test_string)


  # Function for Tokenizing words
  def tokenized_words(X_data):
    words = nltk.word_tokenize(X_data)
    return words

  X_train_data_tokenized_words = []
  for i in range(0, len(X_train_data)):
    test_string = tokenized_words(X_train_data_clear_stopwords[i])
    X_train_data_tokenized_words.append(test_string)

  X_test_data_tokenized_words = []
  for i in range(0, len(X_test_data)):
    test_string = tokenized_words(X_test_data_clear_stopwords[i])
    X_test_data_tokenized_words.append(test_string)


  # Function for Lemmatization
  lemmatizer = WordNetLemmatizer()
  def lematized_words(X_data):
    words = lemmatizer.lemmatize(X_data)
    return words

  X_train_data_lematized_words = []
  for i in range(0, len(X_train_data)):
    X_train_data_lematized_word = []
    for j in range(0, len(X_train_data_tokenized_words[i])):
      test_string = lematized_words(X_train_data_tokenized_words[i][j])
      X_train_data_lematized_word.append(test_string)
    X_train_data_lematized = ' '.join(X_train_data_lematized_word)
    X_train_data_lematized_words.append(X_train_data_lematized)

  X_test_data_lematized_words = []
  for i in range(0, len(X_test_data)):
    X_test_data_lematized_word = []
    for j in range(0, len(X_test_data_tokenized_words[i])):
      test_string = lematized_words(X_test_data_tokenized_words[i][j])
      X_test_data_lematized_word.append(test_string)
    X_test_data_lematized = ' '.join(X_test_data_lematized_word)
    X_test_data_lematized_words.append(X_test_data_lematized)


  # Creating the Bag of Words Model
  cv = CountVectorizer(max_features = 1000)

  X_train_data_vector = cv.fit_transform(X_train_data_lematized_words).toarray()
  X_test_data_vector = cv.fit_transform(X_test_data_lematized_words).toarray()


  # Integer counts to weighted TF-IDF scores
  tf = TfidfTransformer()

  X_train_data_tfidf = tf.fit_transform(X_train_data_vector).toarray()
  X_test_data_tfidf = tf.fit_transform(X_test_data_vector).toarray()

  return X_train_data_tfidf, X_test_data_tfidf

"""# Model Selection"""

# Taining and testing data to be used in our model
train_X, test_X = X_final(X_train, X_test)

"""### Multinomial Naive Bayes"""

MNB = MultinomialNB()

MNB.fit(train_X,y_train)

predictions = MNB.predict(test_X)

# Model evaluation
print(classification_report(y_test, predictions))
print(confusion_matrix(y_test, predictions))

MNB_f1 = round(f1_score(y_test, predictions, average= 'weighted'), 3)
MNB_accuracy = round((accuracy_score(y_test, predictions) * 100), 2)

print("Accuracy : " , MNB_accuracy , " %")
print("f1_score : " , MNB_f1)

"""### Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier

rfc=RandomForestClassifier(random_state= seed)

rfc=RandomForestClassifier(n_estimators= 10, random_state= seed)

rfc.fit(train_X,y_train)

predictions = rfc.predict(test_X)

# Model evaluation
print(classification_report(y_test, predictions))
print(confusion_matrix(y_test, predictions))

rfc_f1 = round(f1_score(y_test, predictions, average= 'weighted'), 3)
rfc_accuracy = round((accuracy_score(y_test, predictions) * 100), 2)

print("Accuracy : " , rfc_accuracy , " %")
print("f1_score : " , rfc_f1)

"""### XGBoost"""

xgb_ = xgb.XGBClassifier(
 learning_rate =0.1,
 n_estimators=1000,
 max_depth=5,
 min_child_weight=1,
 gamma=0,
 subsample=0.8,
 colsample_bytree=0.8,
 objective= 'multi:softmax',
 nthread=4,
 scale_pos_weight=1,
 seed= seed)

xgb_.fit(train_X,y_train)

predictions = xgb_.predict(test_X)

# Model evaluation
print(classification_report(y_test, predictions))
print(confusion_matrix(y_test, predictions))

xgb_f1 = round(f1_score(y_test, predictions, average= 'weighted'), 3)
xgb_accuracy = round((accuracy_score(y_test, predictions) * 100), 2)

print("Accuracy : " , xgb_accuracy , " %")
print("f1_score : " , xgb_f1)

"""### Support Vector Machine (SVM)"""

from sklearn import svm

svc = svm.SVC(random_state= seed)

#Using k-fold cross validation technique
kf = KFold(n_splits = 5, random_state= seed)

#Hyperparameter tuning using grid search for SVM

param_grid = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],
                     'C': [1, 10, 100, 1000]},
                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}] #Large Value of parameter C => small margin
                                                                     #Small Value of paramerter C => Large margin

grid = GridSearchCV(estimator= svc, param_grid= param_grid, scoring= 'accuracy', cv= kf)
# fit the train data 
grid.fit(train_X, y_train)

print('Estimator: ', grid.best_estimator_)
print('Best params : \n', grid.best_params_)
print('Output Classes: ', grid.classes_)
print('Training Accuracy: ', grid.best_score_)

# Predictions on test data
predictions = grid.predict(test_X)

# Model evaluation
print(classification_report(y_true= y_test, y_pred= predictions))

svc_f1 = round(f1_score(y_test, predictions, average= 'weighted'), 3)
svc_accuracy = round((accuracy_score(y_test, predictions) * 100), 2)

print("Accuracy : " , svc_accuracy , " %")
print("f1_score : " , svc_f1)

"""### LSTM --> Long Short Term Memory Implementation"""

from keras import Sequential
from keras.layers import Embedding, LSTM, Dense, Dropout

embedding_size =32
max_words = 5000

model=Sequential()
model.add(Embedding(max_words, embedding_size, input_length= train_X.shape[1]))
model.add(LSTM(100))
model.add(Dense(3, activation='softmax'))

print(model.summary())

model.compile(loss='categorical_crossentropy', 
             optimizer='adam', 
             metrics=['accuracy'])

#Converting y_train categorical labels to numbers
y_train_dummies = pd.get_dummies(y_train).values
print('Shape of label tensor:', y_train_dummies.shape)

#Train the model
model.fit(train_X, y_train_dummies, epochs= 5, batch_size= 32) 

model.save('AmazonProductReview.h5')

#Converting y_test categorical labels to numbers
y_test_dummies = pd.get_dummies(y_test).values
print('Shape of label tensor:', y_test_dummies.shape)

# Model evaluation
from keras.models import load_model

model_ = load_model('AmazonProductReview.h5')
scores = model_.evaluate(test_X, y_test_dummies)

LSTM_accuracy = scores[1] * 100
print('Test accuracy:', scores[1] * 100, ' %')

"""# Model Comparision"""

model = ['MNB', 'Random Forest', 'XGBoost', 'SVM', 'LSTM']
acc = [MNB_accuracy, rfc_accuracy, xgb_accuracy, svc_accuracy, LSTM_accuracy]

#Comapring the accuracy for various models
sns.set_style("whitegrid")
plt.figure(figsize=(10,5))
plt.yticks(np.arange(0,100,10))
plt.ylabel("Test Accuracy %")
plt.xlabel("Machine Learning Model")
sns.barplot(x= model, y= acc)
plt.show()

"""Accuracy is not considered to be a good metric for assessing the model’s performance when the dataset is imbalanced.


f1-score is a good metric for assessing the model’s performance when the dataset is imbalanced. The f1-score combines the precision and recall using the harmonic mean.
"""

# Comparing the f1-score for various models
model = ['MNB', 'Random Forest', 'XGBoost', 'SVM']
f1_score = [MNB_f1, rfc_f1, xgb_f1, svc_f1]

sns.set_style("whitegrid")
plt.figure(figsize=(10,8))
plt.yticks(np.linspace(0,1,21))
plt.ylabel("f1-score")
plt.xlabel("Machine Learning Model")
sns.barplot(x= model,  y= f1_score)
plt.show()